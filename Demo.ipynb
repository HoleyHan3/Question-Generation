{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "We'll gather all the functions from the other notebooks and generate questions for an entirely new text.\n",
    "\n",
    "You can also run this notebook independently with any text and see the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "from pathlib import Path\n",
    "\n",
    "def dumpPickle(fileName, content):\n",
    "    pickleFile = open(fileName, 'wb')\n",
    "    cPickle.dump(content, pickleFile, -1)\n",
    "    pickleFile.close()\n",
    "\n",
    "def loadPickle(fileName):    \n",
    "    file = open(fileName, 'rb')\n",
    "    content = cPickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return content\n",
    "    \n",
    "def pickleExists(fileName):\n",
    "    file = Path(fileName)\n",
    "    \n",
    "    if file.is_file():\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Extract all words from plain text and generate it's features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Extract answers and the sentence they are in\n",
    "def extractAnswers(qas, doc):\n",
    "    answers = []\n",
    "\n",
    "    senStart = 0\n",
    "    senId = 0\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        senLen = len(sentence.text)\n",
    "\n",
    "        for answer in qas:\n",
    "            answerStart = answer['answers'][0]['answer_start']\n",
    "\n",
    "            if (answerStart >= senStart and answerStart < (senStart + senLen)):\n",
    "                answers.append({'sentenceId': senId, 'text': answer['answers'][0]['text']})\n",
    "\n",
    "        senStart += senLen\n",
    "        senId += 1\n",
    "    \n",
    "    return answers\n",
    "\n",
    "#TODO - Clean answers from stopwords?\n",
    "def tokenIsAnswer(token, sentenceId, answers):\n",
    "    for i in range(len(answers)):\n",
    "        if (answers[i]['sentenceId'] == sentenceId):\n",
    "            if (answers[i]['text'] == token):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "#Save named entities start points\n",
    "\n",
    "def getNEStartIndexs(doc):\n",
    "    neStarts = {}\n",
    "    for ne in doc.sents:\n",
    "        neStarts[ne.start] = ne\n",
    "        \n",
    "    return neStarts \n",
    "\n",
    "def getSentenceStartIndexes(doc):\n",
    "    senStarts = []\n",
    "    \n",
    "    for sentence in doc.sents:\n",
    "        senStarts.append(sentence[0].i)\n",
    "    \n",
    "    return senStarts\n",
    "    \n",
    "def getSentenceForWordPosition(wordPos, senStarts):\n",
    "    for i in range(1, len(senStarts)):\n",
    "        if (wordPos < senStarts[i]):\n",
    "            return i - 1\n",
    "        \n",
    "def addWordsForParagrapgh(newWords, text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    neStarts = getNEStartIndexs(doc)\n",
    "    senStarts = getSentenceStartIndexes(doc)\n",
    "    \n",
    "    #index of word in spacy doc text\n",
    "    i = 0\n",
    "    \n",
    "    while (i < len(doc)):\n",
    "        #If the token is a start of a Named Entity, add it and push to index to end of the NE\n",
    "        if (i in neStarts):\n",
    "            word = neStarts[i]\n",
    "            #add word\n",
    "            currentSentence = getSentenceForWordPosition(word.start, senStarts)\n",
    "            wordLen = word.end - word.start\n",
    "            shape = ''\n",
    "            for wordIndex in range(word.start, word.end):\n",
    "                shape += (' ' + doc[wordIndex].shape_)\n",
    "\n",
    "            newWords.append([word.text,\n",
    "                            0,\n",
    "                            0,\n",
    "                            currentSentence,\n",
    "                            wordLen,\n",
    "                            word.label_,\n",
    "                            None,\n",
    "                            None,\n",
    "                            None,\n",
    "                            shape])\n",
    "            i = neStarts[i].end - 1\n",
    "        #If not a NE, add the word if it's not a stopword or a non-alpha (not regular letters)\n",
    "        else:\n",
    "            if (doc[i].is_stop == False and doc[i].is_alpha == True):\n",
    "                word = doc[i]\n",
    "\n",
    "                currentSentence = getSentenceForWordPosition(i, senStarts)\n",
    "                wordLen = 1\n",
    "\n",
    "                newWords.append([word.text,\n",
    "                                0,\n",
    "                                0,\n",
    "                                currentSentence,\n",
    "                                wordLen,\n",
    "                                None,\n",
    "                                word.pos_,\n",
    "                                word.tag_,\n",
    "                                word.dep_,\n",
    "                                word.shape_])\n",
    "        i += 1\n",
    "\n",
    "def oneHotEncodeColumns(df):\n",
    "    columnsToEncode = ['NER', 'POS', \"TAG\", 'DEP']\n",
    "\n",
    "    for column in columnsToEncode:\n",
    "        one_hot = pd.get_dummies(df[column])\n",
    "        one_hot = one_hot.add_prefix(column + '_')\n",
    "\n",
    "        df = df.drop(column, axis = 1)\n",
    "        df = df.join(one_hot)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Predict whether a word is a keyword* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDf(text):\n",
    "    words = []\n",
    "    addWordsForParagrapgh(words, text)\n",
    "\n",
    "    wordColums = ['text', 'titleId', 'paragrapghId', 'sentenceId','wordCount', 'NER', 'POS', 'TAG', 'DEP','shape']\n",
    "    df = pd.DataFrame(words, columns=wordColums)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDf(df):\n",
    "    #One-hot encoding\n",
    "    wordsDf = oneHotEncodeColumns(df)\n",
    "\n",
    "\n",
    "    #Add missing colums \n",
    "    predictorFeaturesName = 'data/pickles/nb-predictor-features.pkl'\n",
    "    featureNames = loadPickle(predictorFeaturesName)\n",
    "\n",
    "    for feature in featureNames:\n",
    "        if feature not in wordsDf.columns:\n",
    "            wordsDf[feature] = 0    \n",
    "                \n",
    "    #Drop unused columns\n",
    "    columnsToDrop = ['text', 'titleId', 'paragrapghId', 'sentenceId', 'shape', 'isAnswer']\n",
    "    wordsDf = wordsDf.drop(columnsToDrop, axis = 1)\n",
    "\n",
    "\n",
    "    return wordsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictWords(wordsDf, df):\n",
    "    \n",
    "    predictorPickleName = 'data/pickles/nb-predictor.pkl'\n",
    "    predictor = loadPickle(predictorPickleName)    \n",
    "    y_pred = predictor.predict_proba(wordsDf)\n",
    "\n",
    "    labeledAnswers = []\n",
    "    for i in range(len(y_pred)):\n",
    "        labeledAnswers.append({'word': df.iloc[i]['text'], 'prob': y_pred[i][0]})\n",
    "    \n",
    "    return labeledAnswers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Extract questions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blankAnswer(firstTokenIndex, lastTokenIndex, sentStart, sentEnd, doc):\n",
    "    leftPartStart = doc[sentStart].idx\n",
    "    leftPartEnd = doc[firstTokenIndex].idx\n",
    "    rightPartStart = doc[lastTokenIndex].idx + len(doc[lastTokenIndex])\n",
    "    rightPartEnd = doc[sentEnd - 1].idx + len(doc[sentEnd - 1])\n",
    "    \n",
    "    question = doc.text[leftPartStart:leftPartEnd] + '_____' + doc.text[rightPartStart:rightPartEnd]\n",
    "    \n",
    "    return question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addQuestions(answers, text):\n",
    "    doc = nlp(text)\n",
    "    currAnswerIndex = 0\n",
    "    qaPair = []\n",
    "\n",
    "    #Check wheter each token is the next answer\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            \n",
    "            #If all the answers have been found, stop looking\n",
    "            if currAnswerIndex >= len(answers):\n",
    "                break\n",
    "            \n",
    "            #In the case where the answer is consisted of more than one token, check the following tokens as well.\n",
    "            answerDoc = nlp(answers[currAnswerIndex]['word'])\n",
    "            answerIsFound = True\n",
    "            \n",
    "            for j in range(len(answerDoc)):\n",
    "                if token.i + j >= len(doc) or doc[token.i + j].text != answerDoc[j].text:\n",
    "                    answerIsFound = False\n",
    "           \n",
    "            #If the current token is corresponding with the answer, add it \n",
    "            if answerIsFound:\n",
    "                question = blankAnswer(token.i, token.i + len(answerDoc) - 1, sent.start, sent.end, doc)\n",
    "                \n",
    "                qaPair.append({'question' : question, 'answer': answers[currAnswerIndex]['word'], 'prob': answers[currAnswerIndex]['prob']})\n",
    "                \n",
    "                currAnswerIndex += 1\n",
    "                \n",
    "    return qaPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortAnswers(qaPairs):\n",
    "    orderedQaPairs = sorted(qaPairs, key=lambda qaPair: qaPair['prob'])\n",
    "    \n",
    "    return orderedQaPairs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Distractors*\n",
    "Taken from the *04. Generating incorrect answers/Incorrect-answers* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6r/s1hxhpyj49lbzk9f_7vc3krh0000gn/T/ipykernel_18249/3940958674.py:13: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, tmp_file)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "#import gensim.models.keyedvectors as word2vec\n",
    "\n",
    "glove_file = 'data/embeddings/glove.6B.300d.txt'\n",
    "tmp_file = 'data/embeddings/word2vec-glove.6B.300d.txt'\n",
    "model = None\n",
    "\n",
    "if os.path.isfile(glove_file):\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "else:\n",
    "    print(\"Glove embeddings not found. Please download and place them in the following path: \" + glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distractors(answer, count):\n",
    "    answer = str.lower(answer)\n",
    "    \n",
    "    ##Extracting closest words for the answer. \n",
    "    try:\n",
    "        closestWords = model.most_similar(positive=[answer], topn=count)\n",
    "    except:\n",
    "        #In case the word is not in the vocabulary, or other problem not loading embeddings\n",
    "        return []\n",
    "\n",
    "    #Return count many distractors\n",
    "    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n",
    "    \n",
    "    return distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDistractors(qaPairs, count):\n",
    "    if not model:\n",
    "        print(\"Glove embeddings not found. Please download and place them in the following path: \" + glove_file)\n",
    "    \n",
    "    for qaPair in qaPairs:\n",
    "        distractors = generate_distractors(qaPair['answer'], count)\n",
    "        qaPair['distractors'] = distractors\n",
    "    \n",
    "    return qaPairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateQuestions(text, count):\n",
    "    \n",
    "    # Extract words \n",
    "    df = generateDf(text)\n",
    "    wordsDf = prepareDf(df)\n",
    "    \n",
    "    # Predict \n",
    "    labeledAnswers = predictWords(wordsDf, df)\n",
    "    \n",
    "    # Transform questions\n",
    "    qaPairs = addQuestions(labeledAnswers, text)\n",
    "    \n",
    "    # Pick the best questions\n",
    "    orderedQaPairs = sortAnswers(qaPairs)\n",
    "    \n",
    "    # Generate distractors\n",
    "    questions = addDistractors(orderedQaPairs[:count], 4)\n",
    "    \n",
    "    # Print\n",
    "    for i in range(count):\n",
    "        display(Markdown('### Question ' + str(i + 1) + ':'))\n",
    "        print(questions[i]['question'])\n",
    "\n",
    "        display(Markdown('#### Answer:'))\n",
    "        print(questions[i]['answer'])\n",
    "        \n",
    "        display(Markdown('#### Incorrect answers:'))\n",
    "        for distractor in questions[i]['distractors']:\n",
    "            print(distractor)\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/han.med/Coding/Question-Generation/.venv/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator GaussianNB from version 0.24.1 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/han.med/Coding/Question-Generation/.venv/lib/python3.11/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 78 features, but GaussianNB is expecting 77 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table, a highly reactive nonmetal, and an oxidizing agent that readily forms oxides with most elements as well as with other compounds. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O2. Diatomic oxygen gas constitutes 20.8\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mf the Earth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms atmosphere. As compounds including oxides, the element makes up almost half of the Earth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms crust.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgenerateQuestions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 8\u001b[0m, in \u001b[0;36mgenerateQuestions\u001b[0;34m(text, count)\u001b[0m\n\u001b[1;32m      5\u001b[0m wordsDf \u001b[38;5;241m=\u001b[39m prepareDf(df)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m labeledAnswers \u001b[38;5;241m=\u001b[39m \u001b[43mpredictWords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwordsDf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Transform questions\u001b[39;00m\n\u001b[1;32m     11\u001b[0m qaPairs \u001b[38;5;241m=\u001b[39m addQuestions(labeledAnswers, text)\n",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m, in \u001b[0;36mpredictWords\u001b[0;34m(wordsDf, df)\u001b[0m\n\u001b[1;32m      3\u001b[0m predictorPickleName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/pickles/nb-predictor.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m predictor \u001b[38;5;241m=\u001b[39m loadPickle(predictorPickleName)    \n\u001b[0;32m----> 5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwordsDf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m labeledAnswers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_pred)):\n",
      "File \u001b[0;32m~/Coding/Question-Generation/.venv/lib/python3.11/site-packages/sklearn/naive_bayes.py:144\u001b[0m, in \u001b[0;36m_BaseNB.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    Return probability estimates for the test vector X.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m        order, as they appear in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_log_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Coding/Question-Generation/.venv/lib/python3.11/site-packages/sklearn/naive_bayes.py:122\u001b[0m, in \u001b[0;36m_BaseNB.predict_log_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mReturn log-probability estimates for the test vector X.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    order, as they appear in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# normalize by P(x) = P(f_1, ..., f_n)\u001b[39;00m\n",
      "File \u001b[0;32m~/Coding/Question-Generation/.venv/lib/python3.11/site-packages/sklearn/naive_bayes.py:269\u001b[0m, in \u001b[0;36mGaussianNB._check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding/Question-Generation/.venv/lib/python3.11/site-packages/sklearn/base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Coding/Question-Generation/.venv/lib/python3.11/site-packages/sklearn/base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 78 features, but GaussianNB is expecting 77 features as input."
     ]
    }
   ],
   "source": [
    "text = \"Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table, a highly reactive nonmetal, and an oxidizing agent that readily forms oxides with most elements as well as with other compounds. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O2. Diatomic oxygen gas constitutes 20.8% of the Earth's atmosphere. As compounds including oxides, the element makes up almost half of the Earth's crust.\"\n",
    "\n",
    "generateQuestions(text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
